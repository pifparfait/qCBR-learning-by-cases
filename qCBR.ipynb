{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qCBR, learning by cases\n",
    "\n",
    "A supervised classifier can be interpreted as a program capable of predicting the output of input from the labelled data. From the input data and the label, with known examples, we will be able to, which will be the label given new input.\n",
    "\n",
    "The main idea of quantum Case-Based Reasoning (qCBR)[1] is to interpret the statement of the problem as an input and the solution to the problem as an output. Therefore, if we have a series of situations with their outcomes, we can train our classifier to determine its solution given a new problem.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pifparfait/qCBR/master/qCBR_1.png\" width=300 height=600 class="center"/>\n",
    "\n",
    "The first step is to determine how to code a problem for input. In the case of Maxcut, the statement of a problem is perfectly defined by its adjacency matrix; therefore, we can have this matrix as the statement of the problem, which will be the input of our qCBR.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pifparfait/qCBR/master/qCBR_2.png\" width=600 height=600 class="center" />\n",
    "\n",
    "\n",
    "We will only focus on the classifier throughout this notebook, but the qCBR follows a slightly more complicated process following human reasoning.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pifparfait/qCBR/master/qCBR_B.png\" class="center"  />\n",
    "\n",
    "The qCBR cycle can be summarised in four steps: \n",
    "- (1) **Retrieval** of the most similar cases, \n",
    "- (2) **Re-use (Adaptation)** to those cases to propose a new solution to the new environment, \n",
    "- (3) **Revise (Validity)** check of the proposed solution and finally, \n",
    "- (4) **Retain (Storage)** following a learning policy.\n",
    "\n",
    "\n",
    "If one of the data that we find in the memory of the cases, we can interpret it to solve the problem or with a certain probability, we solve it to see if the solution ends up improving.\n",
    "\n",
    "\n",
    "[1]: https://arxiv.org/abs/2104.00409 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'# making nice plot\n",
    "\n",
    "qCBR_input = np.load(\"qCBR_input.npy\")\n",
    "qCBR_output = np.load(\"qCBR_output.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The qCBR classifier.\n",
    "\n",
    "We have designed a classifier that emulates neural networks solving the function $Wx + b$, with $W$ and $b$ the parameters and $x$, the sample data to be classified. The non-linearity of the quantum gates is used to implement the activation function $f (Wx + b)$ given $Wx + b$. The figure provides us with the block diagram of the classifier. The optimization and parameters' $(W, b)$ actualization are done in the first step, MSE between ($\\bar y$ and $y$), where $y$ is the label associated with $x$ and $k$ of the labels.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pifparfait/qCBR/master/Classifiers_Gen.png\" width=600 height=600 class="center" />\n",
    "\n",
    "The detailed operations of the classifier are given by the figure where the quantum gates, $R_{y}$, $R_{x}$ and $C_{RZ}$ are used to define the block.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pifparfait/qCBR/master/Classifiers_details.png\" class="center" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 70 #Number of epochs\n",
    "\n",
    "p = 0.5  # probability of an edge\n",
    "seed = 396 #400 392 200\n",
    "\n",
    "n_nodes = 5\n",
    "n_samples = len(qCBR_input)\n",
    "n_test = int(0.1 * n_samples)\n",
    "\n",
    "n_blocks = 2 # Number of blocks\n",
    "n_labels = 2**n_nodes # number of labels\n",
    "n_features = n_nodes**2 # dataset attributes\n",
    "n_qubits = n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(w, b, x):\n",
    "    # We define the block\n",
    "    #print (\"Printing x\", x)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(np.dot(w[i], x) + b[i], wires = i)\n",
    "        qml.RZ(np.dot(w[i], x) + b[i], wires = i)\n",
    "        qml.CRZ(np.dot(w[i], x) + b[i], wires = [i, (i-1) % n_qubits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Definition of our qCBR variational classifier circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires = n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_qCBR(W,B,x):\n",
    "    # This will be the entire circuit\n",
    "    for i in range(n_blocks):\n",
    "        layers(W[i], B[i], x)\n",
    "    return [qml.probs(wires = i) for i in range(n_qubits)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of our cost function. In this demo, we are using the MSE, but we can use another more appropriate to the problem we need to solve. \n",
    "\n",
    "This error can be improved by playing with the corresponding energy with the solution;  however, we rely on the MSE in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_error(W, B, x, labels):\n",
    "    err = 0\n",
    "    for i_data, i_label in zip(x, labels):\n",
    "        err += error(W, B, i_data, i_label)\n",
    "    return err / len(labels)\n",
    "\n",
    "def error(W,B,i_data, i_label):\n",
    "    sol = np.array([pr[0] for pr in circuit_qCBR(W, B, i_data)], requires_grad = True)\n",
    "    out =  np.square(np.subtract(sol, i_label)).mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "The learning process and at the end, we plot the train and test graphs.\n",
    "After the previous steps, now we need to pick an optimizer and run the standard optimization loop. In this tutorial we use the *GradientDescentOptimizer* but the user could find more option here: [optimizers in pennylane](https://pennylane.readthedocs.io/en/stable/introduction/optimizers.html \"optimizers\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comparative_graphs(x_train, labels_train, x_test, labels_test):\n",
    "    \n",
    "    #opt = qml.GradientDescentOptimizer(stepsize = 0.22)\n",
    "    opt=  qml.AdamOptimizer(stepsize=0.01, beta1=0.9, beta2=0.99, eps=1e-08)\n",
    "    epochs = NUM_EPOCHS\n",
    "\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    x_train = np.array(x_train, requires_grad = False)\n",
    "    labels_train = np.array(labels_train, requires_grad = False)\n",
    "    W = np.random.rand(n_blocks, n_qubits, n_features, requires_grad = True)\n",
    "    B = np.random.rand(n_blocks, n_qubits, requires_grad = True )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        W, B, _, _ = opt.step(total_error, W, B, x_train, labels_train)\n",
    "        train_error.append(total_error(W,B,x_train, labels_train))\n",
    "        test_error.append(total_error(W,B,x_test, labels_test))\n",
    "        res = [epoch + 1, train_error[epoch], test_error[epoch], 1-train_error[epoch],1-test_error[epoch]]\n",
    "        print(\"Epoch: {:2d} | Train error: {:3f} | Test error: {:3f} | Train accuracy: {:3f} | Test accuracy: {:3f}\".format(*res))\n",
    "        \n",
    "    plt.style.use(\"seaborn\")\n",
    "    plt.plot(train_error, label = \"train error\")\n",
    "    plt.plot(test_error, label = \"test error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(qCBR_dataset, labels, test_size=0.25):\n",
    "    N_TEST = int(n_samples * test_size)\n",
    "\n",
    "    x_test = qCBR_dataset[:n_test]\n",
    "    x_train = qCBR_dataset[n_test:]\n",
    "    labels_test = labels[:n_test]\n",
    "    labels_train = labels[n_test:]\n",
    "\n",
    "    return x_train, x_test, labels_train, labels_test\n",
    "    \n",
    "x_train, x_test, labels_train, labels_test = split_train_test(qCBR_input, qCBR_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us train our classifier and prepare it for the test.\n",
    "First, we compare our test and train error, and later we plot the two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Train error: 0.323482 | Test error: 0.370004 | Train accuracy: 0.676518 | Test accuracy: 0.629996\n",
      "Epoch:  2 | Train error: 0.304458 | Test error: 0.346477 | Train accuracy: 0.695542 | Test accuracy: 0.653523\n",
      "Epoch:  3 | Train error: 0.286213 | Test error: 0.321067 | Train accuracy: 0.713787 | Test accuracy: 0.678933\n",
      "Epoch:  4 | Train error: 0.269217 | Test error: 0.295193 | Train accuracy: 0.730783 | Test accuracy: 0.704807\n",
      "Epoch:  5 | Train error: 0.253754 | Test error: 0.270594 | Train accuracy: 0.746246 | Test accuracy: 0.729406\n",
      "Epoch:  6 | Train error: 0.239902 | Test error: 0.248989 | Train accuracy: 0.760098 | Test accuracy: 0.751011\n",
      "Epoch:  7 | Train error: 0.227620 | Test error: 0.231981 | Train accuracy: 0.772380 | Test accuracy: 0.768019\n",
      "Epoch:  8 | Train error: 0.216984 | Test error: 0.220580 | Train accuracy: 0.783016 | Test accuracy: 0.779420\n",
      "Epoch:  9 | Train error: 0.208300 | Test error: 0.214638 | Train accuracy: 0.791700 | Test accuracy: 0.785362\n",
      "Epoch: 10 | Train error: 0.201769 | Test error: 0.212474 | Train accuracy: 0.798231 | Test accuracy: 0.787526\n",
      "Epoch: 11 | Train error: 0.197026 | Test error: 0.211408 | Train accuracy: 0.802974 | Test accuracy: 0.788592\n",
      "Epoch: 12 | Train error: 0.193347 | Test error: 0.209557 | Train accuracy: 0.806653 | Test accuracy: 0.790443\n",
      "Epoch: 13 | Train error: 0.190166 | Test error: 0.206582 | Train accuracy: 0.809834 | Test accuracy: 0.793418\n",
      "Epoch: 14 | Train error: 0.187145 | Test error: 0.202917 | Train accuracy: 0.812855 | Test accuracy: 0.797083\n",
      "Epoch: 15 | Train error: 0.184075 | Test error: 0.199115 | Train accuracy: 0.815925 | Test accuracy: 0.800885\n",
      "Epoch: 16 | Train error: 0.180830 | Test error: 0.195571 | Train accuracy: 0.819170 | Test accuracy: 0.804429\n",
      "Epoch: 17 | Train error: 0.177349 | Test error: 0.192468 | Train accuracy: 0.822651 | Test accuracy: 0.807532\n",
      "Epoch: 18 | Train error: 0.173606 | Test error: 0.189817 | Train accuracy: 0.826394 | Test accuracy: 0.810183\n",
      "Epoch: 19 | Train error: 0.169602 | Test error: 0.187514 | Train accuracy: 0.830398 | Test accuracy: 0.812486\n",
      "Epoch: 20 | Train error: 0.165361 | Test error: 0.185415 | Train accuracy: 0.834639 | Test accuracy: 0.814585\n",
      "Epoch: 21 | Train error: 0.160938 | Test error: 0.183396 | Train accuracy: 0.839062 | Test accuracy: 0.816604\n",
      "Epoch: 22 | Train error: 0.156415 | Test error: 0.181392 | Train accuracy: 0.843585 | Test accuracy: 0.818608\n",
      "Epoch: 23 | Train error: 0.151902 | Test error: 0.179399 | Train accuracy: 0.848098 | Test accuracy: 0.820601\n",
      "Epoch: 24 | Train error: 0.147523 | Test error: 0.177459 | Train accuracy: 0.852477 | Test accuracy: 0.822541\n",
      "Epoch: 25 | Train error: 0.143398 | Test error: 0.175643 | Train accuracy: 0.856602 | Test accuracy: 0.824357\n",
      "Epoch: 26 | Train error: 0.139621 | Test error: 0.174037 | Train accuracy: 0.860379 | Test accuracy: 0.825963\n",
      "Epoch: 27 | Train error: 0.136229 | Test error: 0.172723 | Train accuracy: 0.863771 | Test accuracy: 0.827277\n",
      "Epoch: 28 | Train error: 0.133202 | Test error: 0.171765 | Train accuracy: 0.866798 | Test accuracy: 0.828235\n",
      "Epoch: 29 | Train error: 0.130473 | Test error: 0.171193 | Train accuracy: 0.869527 | Test accuracy: 0.828807\n",
      "Epoch: 30 | Train error: 0.127969 | Test error: 0.170994 | Train accuracy: 0.872031 | Test accuracy: 0.829006\n",
      "Epoch: 31 | Train error: 0.125640 | Test error: 0.171116 | Train accuracy: 0.874360 | Test accuracy: 0.828884\n",
      "Epoch: 32 | Train error: 0.123469 | Test error: 0.171466 | Train accuracy: 0.876531 | Test accuracy: 0.828534\n",
      "Epoch: 33 | Train error: 0.121460 | Test error: 0.171913 | Train accuracy: 0.878540 | Test accuracy: 0.828087\n",
      "Epoch: 34 | Train error: 0.119622 | Test error: 0.172302 | Train accuracy: 0.880378 | Test accuracy: 0.827698\n",
      "Epoch: 35 | Train error: 0.117950 | Test error: 0.172470 | Train accuracy: 0.882050 | Test accuracy: 0.827530\n",
      "Epoch: 36 | Train error: 0.116426 | Test error: 0.172283 | Train accuracy: 0.883574 | Test accuracy: 0.827717\n",
      "Epoch: 37 | Train error: 0.115024 | Test error: 0.171653 | Train accuracy: 0.884976 | Test accuracy: 0.828347\n",
      "Epoch: 38 | Train error: 0.113727 | Test error: 0.170559 | Train accuracy: 0.886273 | Test accuracy: 0.829441\n",
      "Epoch: 39 | Train error: 0.112531 | Test error: 0.169044 | Train accuracy: 0.887469 | Test accuracy: 0.830956\n",
      "Epoch: 40 | Train error: 0.111438 | Test error: 0.167199 | Train accuracy: 0.888562 | Test accuracy: 0.832801\n",
      "Epoch: 41 | Train error: 0.110448 | Test error: 0.165139 | Train accuracy: 0.889552 | Test accuracy: 0.834861\n",
      "Epoch: 42 | Train error: 0.109550 | Test error: 0.162986 | Train accuracy: 0.890450 | Test accuracy: 0.837014\n",
      "Epoch: 43 | Train error: 0.108728 | Test error: 0.160849 | Train accuracy: 0.891272 | Test accuracy: 0.839151\n",
      "Epoch: 44 | Train error: 0.107961 | Test error: 0.158813 | Train accuracy: 0.892039 | Test accuracy: 0.841187\n",
      "Epoch: 45 | Train error: 0.107231 | Test error: 0.156931 | Train accuracy: 0.892769 | Test accuracy: 0.843069\n",
      "Epoch: 46 | Train error: 0.106521 | Test error: 0.155228 | Train accuracy: 0.893479 | Test accuracy: 0.844772\n",
      "Epoch: 47 | Train error: 0.105813 | Test error: 0.153710 | Train accuracy: 0.894187 | Test accuracy: 0.846290\n",
      "Epoch: 48 | Train error: 0.105091 | Test error: 0.152372 | Train accuracy: 0.894909 | Test accuracy: 0.847628\n",
      "Epoch: 49 | Train error: 0.104345 | Test error: 0.151205 | Train accuracy: 0.895655 | Test accuracy: 0.848795\n",
      "Epoch: 50 | Train error: 0.103573 | Test error: 0.150200 | Train accuracy: 0.896427 | Test accuracy: 0.849800\n"
     ]
    }
   ],
   "source": [
    "W,B = Comparative_graphs(x_train, labels_train, x_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will not implement the whole synthesizer block. We only test if the predicted new value is well retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Problem\n",
    "p = 0.5  # probability of an edge\n",
    "g_new = nx.erdos_renyi_graph(n_nodes, p=p,seed=8)\n",
    "positions_new = nx.spring_layout(g_new)\n",
    "\n",
    "def Transform_Label_To_Node (tmp_label):\n",
    "    labels_nodes_tmp = []\n",
    "    for i in range(len(tmp_label)):\n",
    "        if tmp_label[i] == \"1\":\n",
    "            labels_nodes_tmp.append(i)\n",
    "            \n",
    "    return labels_nodes_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = [bin(i)[2:].zfill(n_nodes) for i in range((2**n_nodes))]\n",
    "\n",
    "\n",
    "def max_prob_sol():\n",
    "    tmp =0\n",
    "    tp = 0\n",
    "    for i in range (n_nodes):\n",
    "        prob_circuit_new_tmp = circuit_qCBR(W,B,x_test[i])\n",
    "        out_new_X_tmp = np.argmax(prob_circuit_new_tmp)\n",
    "        index_label_new = labels_map[out_new_X_tmp]\n",
    "        qCBR_labels_nodes_tmp = Transform_Label_To_Node (index_label_new)\n",
    "        tp = len(qCBR_labels_nodes_tmp)\n",
    "        if(tmp<= tp):\n",
    "            tmp =  tp\n",
    "            qCBR_labels_nodes=qCBR_labels_nodes_tmp\n",
    "            \n",
    "    return qCBR_labels_nodes\n",
    "\n",
    "max_prob_sol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the solution from the qCBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = g_new.subgraph(max_prob_sol())\n",
    "nx.draw(g_new, pos=positions_new, with_labels=True)\n",
    "nx.draw(sub, pos=positions_new, node_color=\"r\", edge_color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "If the end-user wants to change the input data, he can go to the root directory. Otherwise, it could generate its own problem. You should only encode the issue in an adjacent matrix, pass it in a flattened vector and provide the associated solution. In the case of wanting to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "[1] https://arxiv.org/abs/2104.00409 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
